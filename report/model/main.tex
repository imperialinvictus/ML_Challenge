\section{Model}
% A description of the model(s) that you are evaluating/exploring. We are expecting a thorough
% exploration of at least 3+ family of models1, even if you don’t ultimately use those models. We are looking for:
% – How you are applying these models. You don’t need to reiterate what the models are and how they work. Instead, we’re looking for a description of the choices you are making to apply the model to this task: e.g., what features from the “Data” section are you using for each model? What adjustments (if any) did you need to make?

\subsection{Neural Network}
We implemented a feedforward neural network with two hidden layers consisting of 64 and 32 neurons, respectively. The hidden layers utilized ReLU activation functions, while the output layer employed a softmax activation function. The model was trained using a learning rate of 0.001 and a batch size of 32, with categorical cross-entropy as the loss function. Training was conducted over 300 epochs, achieving a validation accuracy of approximately 85\%. For feature selection, we used word similarity embeddings for questions Q5, Q6, and Q7. For Q1, Q2, and Q4, we computed the average of all numerical values in the text, and for Q3 and Q8, we employed one-hot encoded categorical vectors.

\subsection{Linear Regression}
We also evaluated a linear regression model using binary cross-entropy as the loss function. The implementation leveraged sklearn's Linear Regression module, utilizing the same feature set as the neural network. The model achieved a validation accuracy of approximately 86\%, demonstrating competitive performance. This comparison with the neural network revealed that the simpler linear regression model performed slightly better, suggesting that the features representing the dataset may not exhibit significant complexity.

\subsection{Decision Trees}
I tested a regular decision tree, an ensemble of decision trees, and a Random Forest model, using the built-in RandomForestClassifier from sklearn. I found the decision tree to perform the worst, and it was easier to manipulate the RandomForestClassifier’s parameters, so I decided to explore that further.

I decided to include all of the possible features (even initially 'id' accidentally), as the Random Forest was very robust regardless of the feature choice or hyperparameters. To allow the text-based features to generalize as well as possible, I used a library to cluster text by Levenshtein distance, and to limit overfitting, I only included the categories with enough representatives. This did not seem to have too large an impact on validation accuracy, but would allow it to correctly categorize otherwise unseen data with unique typos or spelling choices.
I tested many of the hyperparameter options, but found little change in accuracy aside from increasing the number of estimators to around 250, and limiting the minimum samples split to 10. Some randomness was introduced when generating text clusters, but I found it to still be very consistently accurate regardless of what data it trained on.

It consistently achieved an 87\% validation accuracy. The classification report initially showed it was particularly effective at identifying Pizza and Shawarma, but had some difficulty with Sushi, which I would expect if respondents have less familiarity with Sushi. However, after splitting the training data such that there is the same amount of each in the training and validation portions, this was no longer the case.
\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \hline
        Category & Precision & Recall & F1-score \\ 
        \hline
        Pizza    & 0.85 & 0.93 & 0.89 \\
        Shawarma & 0.88 & 0.84 & 0.86 \\
        Sushi    & 0.90 & 0.86 & 0.88 \\
        \hline
        Accuracy      & \multicolumn{3}{c}{0.88 (263 samples)} \\
        Macro avg     & 0.88 & 0.88 & 0.88 \\
        Weighted avg  & 0.88 & 0.88 & 0.88 \\
        \hline
    \end{tabular}
    \caption{Classification Report}
    \label{tab:classification_report}
\end{table}

After comparing the loss, bias, variance decomposition to our other models, I found the expected higher bias, but lower variance that Random Forests are known to have:
\begin{table}[h]
    \centering
    \begin{tabular}{ccc}
        \hline
        Loss & Bias & Variance \\ 
        \hline
        0.1380 & 0.1255 & 0.0410 \\
        \hline
    \end{tabular}
    \label{tab:loss_report}
\end{table}
