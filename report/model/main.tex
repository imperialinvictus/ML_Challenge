\section{Models}
% A description of the model(s) that you are evaluating/exploring. We are expecting a thorough
% exploration of at least 3+ family of models1, even if you don’t ultimately use those models. We are looking for:
% – How you are applying these models. You don’t need to reiterate what the models are and how they work. Instead, we’re looking for a description of the choices you are making to apply the model to this task: e.g., what features from the “Data” section are you using for each model? What adjustments (if any) did you need to make?

The setup of our training is similar for all models:
% Make a bullet point list
\begin{itemize}
    \item We used the cleaned and encoded data as described in \ref{sec:exploration}.
    \item We split the data into training, validation and test sets using a 60/20/20 split using \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html}{train\_test\_split} from sklearn. We used a consistent random seed of 42 to ensure reproducibility.
    \item We used stratified sampling to ensure that the distribution of classes is similar in both sets.
    \item We used the same evaluation metrics for all models: accuracy, precision, recall, and F1-score.
    \item We used \href{https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/}{bias\_variance\_decomp} from mlxtend to calculate the bias and variance of our models.
    \item We used \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html}{classification\_report} from sklearn to generate the classification report for our models.

\subsection{Logistic Regression}

\subsection{Neural Network}

We started with training a MLPClassifier on the cleaned and encoded data as described in \ref{sec:exploration} using the default parameters by 
\href{https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html}{sklearn}. 
The default configuration is as follows:
Hidden layer: 1 layer, 100 hidden units
Activation: ReLu
We achieved an 90\% validation accuracy with this base configuration without any tuning, and 87\% on the test set.
We have also tested bagged models and normalization. Specifically, we used the sklearn \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html}{BaggingClassifier} 
with MLPClassifier as the estimator with 10 estimators. As we learned in class, we expected the bagged model to have lower variance because the prediction is the average of many MLPClassifiers. 
This is confirmed when we performed variance bias decomposition as seen in \ref{tab:nn_bias_var}, where the bagged model has a lower variance than the non-bagged model.
We also tried normalizing the data using the \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}{StandardScaler} from sklearn.
We expected normalization to help the model converge faster and prevent overfitting, but it did not have a significant impact on the model accuracy.
We think this is because the data was already cleaned and encoded, and the features were already in a similar range. 
For example, the numerical features like "How much would you expect to pay" already have similar ranges (5-20) as the categorical features like "How many ingredients would you expect to be in the food".

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        Model                     & Expected Loss & Bias   & Variance \\ 
        \hline
        MLP                       & 0.1356        & 0.1155 & 0.0638   \\
        Bagging MLP               & 0.1362        & 0.1277 & 0.0543   \\
        Normalized MLP            & 0.1368        & 0.1185 & 0.0628   \\
        Normalized Bagging MLP    & 0.1384        & 0.1337 & 0.0530   \\
        \hline
    \end{tabular}
    \caption{Loss, Bias, and Variance for Different Models}
    \label{tab:nn_bias_var}
\end{table}

\ref{tab:nn_classification_report} shows the classification report for the MLPClassifier.
The model performed well on the validation set, with an accuracy of 90\% and a balanced precision and recall across all classes.
The F1-score for all classes was around 0.90, indicating a good balance between precision and recall.

\begin{table}[ht]
    \centering
    \begin{tabular}{lcccc}
        \hline
        Category    & Precision & Recall & F1-score & Support \\ 
        \hline
        Pizza       & 0.92      & 0.90   & 0.91     & 88      \\
        Shawarma    & 0.87      & 0.91   & 0.89     & 88      \\
        Sushi       & 0.91      & 0.89   & 0.90     & 87      \\
        \hline
        Accuracy    &           &        & 0.90     & 263     \\
        Macro avg   & 0.90      & 0.90   & 0.90     & 263     \\
        Weighted avg & 0.90     & 0.90   & 0.90     & 263     \\
        \hline
    \end{tabular}
    \caption{Classification Report for Pizza, Shawarma, and Sushi}
    \label{tab:nn_classification_report}
\end{table}


\subsection{Decision Trees}
I tested a regular decision tree, an ensemble of decision trees, and a Random Forest model, using the built-in RandomForestClassifier from sklearn. I found the decision tree to perform the worst, and it was easier to manipulate the RandomForestClassifier’s parameters, so I decided to explore that further.

I decided to include all of the possible features (even initially 'id' accidentally), as the Random Forest was very robust regardless of the feature choice or hyperparameters. To allow the text-based features to generalize as well as possible, I used a library to cluster text by Levenshtein distance, and to limit overfitting, I only included the categories with enough representatives. This did not seem to have too large an impact on validation accuracy, but would allow it to correctly categorize otherwise unseen data with unique typos or spelling choices.
I tested many of the hyperparameter options, but found insignificant change in accuracy aside from increasing the number of estimators to around 250 (with extremely diminishing returns past that point), and limiting the minimum samples split to 10. Some randomness was introduced when generating text clusters, but I found it to still be very consistently accurate regardless of what data it trained on.

It consistently achieved an 87\% validation accuracy. The classification report initially showed it was particularly effective at identifying Pizza and Shawarma, but had some difficulty with Sushi, which I would expect if respondents have less familiarity with Sushi. However, after splitting the training data such that there is the same amount of each in the training and validation portions, this was no longer the case.
\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        Category & Precision & Recall & F1-score \\ 
        \hline
        Pizza    & 0.85 & 0.93 & 0.89 \\
        Shawarma & 0.88 & 0.84 & 0.86 \\
        Sushi    & 0.90 & 0.86 & 0.88 \\
        \hline
        Accuracy      & \multicolumn{3}{c}{0.88 (263 samples)} \\
        Macro avg     & 0.88 & 0.88 & 0.88 \\
        Weighted avg  & 0.88 & 0.88 & 0.88 \\
        \hline
    \end{tabular}
    \caption{Classification Report}
    \label{tab:classification_report}
\end{table}

After comparing the loss, bias, variance decomposition to our other models, I found the expected higher bias, but lower variance that Random Forests are known to have:
\begin{table}[ht]
    \centering
    \begin{tabular}{ccc}
        \hline
        Loss & Bias & Variance \\ 
        \hline
        0.1380 & 0.1255 & 0.0410 \\
        \hline
    \end{tabular}
    \label{tab:loss_report}
\end{table}