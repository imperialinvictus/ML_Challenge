% \section{Model Choice and Hyperparameters}
% How you are determining what model(s) to use in your final pred.py, as well as an exploration of hyperparameters. We are looking for:
% – A convincing explanation of how you ensured that the evaluation metrics for various models are comparable (i.e., are you using a consistent test set?)
% – A clear description of the evaluation metric(s) used to evaluate your model, as well as justification for its use.
% – A description of the hyperparameters that you are tuning, hyperparameter value combinations that you have tried, and the value of the evaluation metric(s) for those hyperparameters. We are not looking for an exhaustive search of all possible hyperparameter combinations, but there should be enough evidence to demonstrate that your hyperparameter choices are reasonable. (Note that just saying “X was the best hyperparameter value based on metric Y out of the hyperparameter values we tried” is not enough. Please present the values of the evaluation metrics for each hyperparameter value to show that your choice was the best one.)
% – A clear description of what your final model choice looks like in the submitted pred.py file.
% – The descriptions should be consistent with the .py and/or .ipynb files that you used while developing your model.
